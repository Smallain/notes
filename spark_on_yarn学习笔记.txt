spark on yarn


1.下载hadoop，scala，java，spark。其中hadoop要与spark的版本对应上。

2.环境配置方面


  .bashrc：


export JAVA_HOME=/opt/java/jdk1.8.0_131
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar


export HADOOP_HOME=/opt/hadoop/hadoop-2.7.3


export SCALA_HOME=/opt/scala/scala-2.12.2

export SPARK_HOME=/opt/spark/spark-2.1.1-bin-hadoop2.7

export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$SCALA_HOME/bin:$SPARK_HOME/bin:$PATH



3.hadoop配置文件

core-site.xml

<configuration>
<property>
	<name>fs.defaultFS</name>
	<value>hdfs://localhost:9000</value>
</property>
<property>
	<name>hadoop.tmp.dir</name>
	<value>/opt/hadoop/hadoop-2.7.3/tmp</value>
</property>
</configuration>



hdfs-site.xml

<configuration>
<property>
	<name>dfs.replication</name>
	<value>1</value>
</property>
</configuration>




mapred-site.xml

<configuration>

    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>

</configuration>



yarn-site.xml


<property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
</property>

<property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>false</value>
</property>

<property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>
</configuration>



4.spark配置文件设置spark-env.sh

export HADOOP_CONF_DIR=/opt/hadoop/hadoop-2.7.3/etc/hadoop
export SPARK_EXECUTOR_INSTANCES=1
export SPARK_EXECUTOR_CORES=1
export SPARK_EXECUTOR_MEMORY=2G
export SPARK_DRIVER_MEMORY=2G



5.初始化hadoop

hdfs namenode -format

6.启动hdfs

./start-dfs.sh
./start-yarn.sh

7.启动spark，以client模式连接yarn

spark-shell --master yarn --deploy-mode client
